{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1XnVhPdZtAQQXYkpCWNgaTSs4nNdgILio",
      "authorship_tag": "ABX9TyMqehvIUrZY3V5sqxOGcRjp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GijsJackersPXL/Smart_systems_Eindoefening/blob/main/Vraag2_SS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "k19dZ3fH3DTN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory to dataset in drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Set the directory path where your images are stored on Google Drive\n",
        "directory_path = '/content/gdrive/MyDrive/Colab_Notebooks/DataSet'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSijFTHD69ba",
        "outputId": "9b1f0177-f707-48df-e451-387fb4ddb6cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an ImageDataGenerator object\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Load the data from your Google Drive\n",
        "train_data_dir = '/content/gdrive/MyDrive/Colab_Notebooks/DataSet/Train'\n",
        "validation_data_dir = '/content/gdrive/MyDrive/Colab_Notebooks/DataSet/Validation'\n",
        "\n",
        "# Use the ImageDataGenerator to generate batches of image data\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(200, 200),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(200, 200),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6DLlr8VCju7",
        "outputId": "80753d43-52e2-446f-8fda-8e97919ec1c2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 66 images belonging to 6 classes.\n",
            "Found 16 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "his will train your model using the training data stored in /content/gdrive/My Drive/data/train, and validate it using the validation data stored in /content/gdrive/My Drive/data/validation. The target_size parameter specifies the size of the input images, and the batch_size parameter specifies the number of samples in each batch. The class_mode parameter specifies the format of the labels for the data. In this case, it is set to 'binary', which means that there are two classes and the labels are binary (0 or 1)."
      ],
      "metadata": {
        "id": "CuHbVKixGjR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define your model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(200, 200, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile your model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train your model using the generator\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=10,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUeKqVeDG6q0",
        "outputId": "6db8df92-3979-468f-b441-3680babb8f33"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 3/10 [========>.....................] - ETA: 2:42 - loss: -249.4457 - accuracy: 0.1212"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 54s 6s/step - loss: -249.4457 - accuracy: 0.1212 - val_loss: -555.3329 - val_accuracy: 0.1875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f221d2fb550>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your data\n",
        "X = train_data_dir # (n_samples, n_features)\n",
        "y = validation_data_dir # (n_samples,)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Convert the data to NumPy arrays\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_val = np.array(X_val)\n",
        "y_val = np.array(y_val)\n"
      ],
      "metadata": {
        "id": "3GDVah47QAqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train your model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Print the accuracy\n",
        "print(history.history['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "k6xAymavOsMv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}